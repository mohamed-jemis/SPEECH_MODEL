{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport math\nimport pickle\nimport re\nimport librosa\nimport librosa.display\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom tqdm import tqdm\nimport soundfile\nfrom keras.layers import Input, Dense, Dropout\nfrom keras.models import Model, Sequential\nfrom tensorflow.keras.optimizers.legacy import Adam\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-20T00:38:17.476680Z","iopub.execute_input":"2023-02-20T00:38:17.477297Z","iopub.status.idle":"2023-02-20T00:38:24.846242Z","shell.execute_reply.started":"2023-02-20T00:38:17.477167Z","shell.execute_reply":"2023-02-20T00:38:24.845003Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Global Variables","metadata":{}},{"cell_type":"code","source":"sr = 44100\n\nemotion_dict = {'ang': 0,\n                'hap': 1,\n                'exc': 2,\n                'sad': 3,\n                'fru': 4,\n                'fea': 5,\n                'sur': 6,\n                'neu': 7,\n                'dis': 8,\n                'xxx': 9,\n                'oth': 9}","metadata":{"execution":{"iopub.status.busy":"2023-02-20T00:38:24.851175Z","iopub.execute_input":"2023-02-20T00:38:24.852313Z","iopub.status.idle":"2023-02-20T00:38:24.857743Z","shell.execute_reply.started":"2023-02-20T00:38:24.852275Z","shell.execute_reply":"2023-02-20T00:38:24.856550Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Reading Labels","metadata":{}},{"cell_type":"code","source":"def read_labels(datapath):\n    min_frame = 999999\n    info_line = re.compile(r'\\[.+\\]\\n', re.IGNORECASE)\n\n    start_times, end_times, wav_file_names, emotions, vals, acts, doms = [], [], [], [], [], [], []\n\n    evaluation_files = [l for l in os.listdir(datapath) if 'Ses' in l]\n    for file in evaluation_files:\n        with open(datapath + file) as f:\n            content = f.read()\n\n        info_lines = re.findall(info_line, content)\n\n        for line in info_lines[1:]:  # the first line is a header\n            start_end_time, wav_file_name, emotion, val_act_dom = line.strip().split('\\t')\n            start_time, end_time = start_end_time[1:-1].split('-')\n\n            val, act, dom = val_act_dom[1:-1].split(',')\n            val, act, dom = float(val), float(act), float(dom)\n\n            start_time, end_time = float(start_time), float(end_time)\n            min_frame = min(min_frame, end_time - start_time)\n\n            start_times.append(start_time)\n            end_times.append(end_time)\n            wav_file_names.append(wav_file_name)\n            emotions.append(emotion)\n            vals.append(val)\n            acts.append(act)\n            doms.append(dom)\n\n    df_iemocap = pd.DataFrame(columns=['start_time', 'end_time', 'wav_file', 'emotion', 'val', 'act', 'dom'])\n\n    df_iemocap['start_time'] = start_times\n    df_iemocap['end_time'] = end_times\n    df_iemocap['wav_file'] = wav_file_names\n    df_iemocap['emotion'] = emotions\n    df_iemocap['val'] = vals\n    df_iemocap['act'] = acts\n    df_iemocap['dom'] = doms\n\n    df_iemocap.to_csv('df_iemocap.csv', index=False)\n\n    return df_iemocap, min_frame","metadata":{"execution":{"iopub.status.busy":"2023-02-20T00:38:24.859418Z","iopub.execute_input":"2023-02-20T00:38:24.860337Z","iopub.status.idle":"2023-02-20T00:38:24.874837Z","shell.execute_reply.started":"2023-02-20T00:38:24.860291Z","shell.execute_reply":"2023-02-20T00:38:24.873576Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Reading audio files","metadata":{}},{"cell_type":"code","source":"def read_audio(wav_file_path, labels_df, min_frame):\n\n    audio_vectors = {}\n    orig_wav_files = os.listdir(wav_file_path)\n    \n    for orig_wav_file in tqdm(orig_wav_files):\n        try:\n            with soundfile.SoundFile(wav_file_path + orig_wav_file) as sound_file:\n                \n                orig_wav_vector = sound_file.read(dtype=\"float32\")\n                sr=sound_file.samplerate\n                \n                #orig_wav_vector, _sr = librosa.load(wav_file_path + orig_wav_file, sr=sr)\n                orig_wav_vector = orig_wav_vector.reshape(-1, 1)\n\n                orig_wav_file, file_format = orig_wav_file.split('.')\n\n                for index, row in labels_df[labels_df['wav_file'].str.contains(orig_wav_file)].iterrows():\n                    frames = []\n                    start_time, end_time, truncated_wav_file_name, emotion, val, act, dom = row['start_time'], row[\n                        'end_time'], row['wav_file'], row['emotion'], row['val'], row['act'], row['dom']\n\n                    start_frame = math.floor(start_time * sr)\n                    end_frame = math.floor(end_time * sr)\n                    \n                    truncated_wav_vector = orig_wav_vector[start_frame:end_frame + 1].reshape(-1, 1)\n                    \n                    for i in range(math.floor((end_time-start_time)/min_frame)):\n                        startf_time = i * min_frame\n                        endf_time = startf_time + min_frame\n                        \n                        start_frame = math.floor(startf_time * sr)\n                        end_frame = math.floor(endf_time * sr)\n                        \n                        frame = truncated_wav_vector[start_frame:end_frame + 1].reshape(-1, 1)\n                        frames.append(frame)\n                    \n                    audio_vectors[truncated_wav_file_name] = frames\n        except:\n            print('')\n\n    with open('audio_vectors1.pkl', 'wb') as f:\n        pickle.dump(audio_vectors, f)\n\n\n    return audio_vectors\n","metadata":{"execution":{"iopub.status.busy":"2023-02-20T00:38:24.879013Z","iopub.execute_input":"2023-02-20T00:38:24.880166Z","iopub.status.idle":"2023-02-20T00:38:24.892607Z","shell.execute_reply.started":"2023-02-20T00:38:24.880122Z","shell.execute_reply":"2023-02-20T00:38:24.891327Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Extracting features","metadata":{}},{"cell_type":"code","source":"def extract_audio_features(audio_vectors, labels_df, emotion_dict):\n    file_name, x, y = [], [], []\n    i=0\n    for index, row in tqdm(labels_df.iterrows()):\n        i+=1\n        wav_file_name = row['wav_file']\n        label = emotion_dict[row['emotion']]\n        audio = audio_vectors[wav_file_name]\n    \n        for frame in audio:\n            mfcc = np.array(librosa.feature.mfcc(y=frame, sr=sr))\n            mfcc_mean = mfcc.mean(axis=1)\n            mfcc_min = mfcc.min(axis=1)\n            mfcc_max = mfcc.max(axis=1)\n            mfcc_feature = np.concatenate( (mfcc_mean, mfcc_min, mfcc_max) )\n     \n            file_name.append(wav_file_name)\n            x.append(mfcc_feature.reshape(1, -1).tolist()[0])\n            y.append(label)\n        \n        if i == 267:\n            break\n    \n    df = pd.concat([pd.DataFrame(file_name, columns=[\"file_name\"]), pd.DataFrame(x), pd.DataFrame(y, columns=[\"label\"])], axis=1)\n    #df.to_csv('audios_features.csv', index=False)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2023-02-20T00:38:24.894500Z","iopub.execute_input":"2023-02-20T00:38:24.894930Z","iopub.status.idle":"2023-02-20T00:38:24.907624Z","shell.execute_reply.started":"2023-02-20T00:38:24.894890Z","shell.execute_reply":"2023-02-20T00:38:24.906225Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Keras Model","metadata":{}},{"cell_type":"code","source":"def create_model(input_shape, n_units):\n    model = Sequential()\n    model.add(Input(shape=input_shape))\n    \n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(64, activation='relu'))\n    \n    model.add(Dropout(0.5))\n \n    model.add(Dense(units=n_units, activation=\"softmax\"))\n    \n    opt = Adam(learning_rate=0.1)\n    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-02-20T00:38:24.908861Z","iopub.execute_input":"2023-02-20T00:38:24.909204Z","iopub.status.idle":"2023-02-20T00:38:24.922674Z","shell.execute_reply.started":"2023-02-20T00:38:24.909175Z","shell.execute_reply":"2023-02-20T00:38:24.921470Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Code and model","metadata":{}},{"cell_type":"code","source":"labels_path = '/kaggle/input/iemocapfullrelease/IEMOCAP_full_release/Session1/dialog/EmoEvaluation/'\nwav_path = '/kaggle/input/iemocapfullrelease/IEMOCAP_full_release/Session1/dialog/wav/'\n# read the pickle and the csv\n\n# labels_df = pd.read_csv(\"/kaggle/working/df_iemocap.csv\")\n# audio_vectors = pickle.load(open('/kaggle/working/audio_vectors1.pkl', 'rb'))\n#labeled_features_df = pd.read_csv(\"/kaggle/working/audios_features.csv\")\n\nlabels_df, min_frame = read_labels(labels_path)\naudio_vectors = read_audio(wav_path, labels_df, min_frame)\nlabeled_features_df = extract_audio_features(audio_vectors, labels_df, emotion_dict)\n\nx = labeled_features_df.drop(columns=['label', 'file_name'])\n\nenc = OneHotEncoder()\ny = enc.fit_transform(np.asarray(labeled_features_df['label']).reshape(-1,1)).toarray()\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n\n#### MLP MODEL\n#model = MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08, hidden_layer_sizes=(300,), learning_rate='adaptive', max_iter=500)\n# # print(x_train[0].shape)\n# x_train = np.array(x_train).reshape(-1,1)\n# y_train = np.array(y_train).reshape(-1,1)\n#print('shape is ',x_train.shape,' ',y_train.shape)\n\n#model.fit(x_train, y_train)\n#y_pred = model.predict(x_test)\n#accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\n#print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n\nprint('create model')\nmodel = create_model((None, x_train.shape[1]), len(y[0]))\nprint('fit model')\n\nmodel.fit(x_train, y_train, epochs = 1000, verbose=2, validation_data=(x_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2023-02-20T00:38:24.925995Z","iopub.execute_input":"2023-02-20T00:38:24.926690Z","iopub.status.idle":"2023-02-20T00:53:14.725228Z","shell.execute_reply.started":"2023-02-20T00:38:24.926646Z","shell.execute_reply":"2023-02-20T00:53:14.724185Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"100%|██████████| 28/28 [00:00<00:00, 33.40it/s]\n0it [00:00, ?it/s]/opt/conda/lib/python3.7/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=2048 is too small for input signal of length=1\n  return f(*args, **kwargs)\n266it [13:45,  3.10s/it]\n","output_type":"stream"},{"name":"stdout","text":"create model\nfit model\nEpoch 1/100\n36/36 - 2s - loss: 4971.0356 - accuracy: 0.2085 - val_loss: 1.9970 - val_accuracy: 0.1979 - 2s/epoch - 53ms/step\nEpoch 2/100\n36/36 - 0s - loss: 1.9428 - accuracy: 0.2233 - val_loss: 1.8848 - val_accuracy: 0.3264 - 356ms/epoch - 10ms/step\nEpoch 3/100\n36/36 - 0s - loss: 1.8597 - accuracy: 0.2815 - val_loss: 1.8189 - val_accuracy: 0.3264 - 429ms/epoch - 12ms/step\nEpoch 4/100\n36/36 - 0s - loss: 1.8163 - accuracy: 0.2815 - val_loss: 1.7869 - val_accuracy: 0.3264 - 351ms/epoch - 10ms/step\nEpoch 5/100\n36/36 - 0s - loss: 1.7955 - accuracy: 0.2815 - val_loss: 1.7703 - val_accuracy: 0.3264 - 357ms/epoch - 10ms/step\nEpoch 6/100\n36/36 - 0s - loss: 1.7858 - accuracy: 0.2815 - val_loss: 1.7621 - val_accuracy: 0.3264 - 356ms/epoch - 10ms/step\nEpoch 7/100\n36/36 - 0s - loss: 1.7811 - accuracy: 0.2815 - val_loss: 1.7580 - val_accuracy: 0.3264 - 356ms/epoch - 10ms/step\nEpoch 8/100\n36/36 - 0s - loss: 1.7778 - accuracy: 0.2815 - val_loss: 1.7561 - val_accuracy: 0.3264 - 361ms/epoch - 10ms/step\nEpoch 9/100\n36/36 - 0s - loss: 1.7767 - accuracy: 0.2815 - val_loss: 1.7543 - val_accuracy: 0.3264 - 349ms/epoch - 10ms/step\nEpoch 10/100\n36/36 - 0s - loss: 1.7755 - accuracy: 0.2815 - val_loss: 1.7524 - val_accuracy: 0.3264 - 332ms/epoch - 9ms/step\nEpoch 11/100\n36/36 - 0s - loss: 1.7750 - accuracy: 0.2815 - val_loss: 1.7525 - val_accuracy: 0.3264 - 343ms/epoch - 10ms/step\nEpoch 12/100\n36/36 - 0s - loss: 1.7744 - accuracy: 0.2815 - val_loss: 1.7524 - val_accuracy: 0.3264 - 430ms/epoch - 12ms/step\nEpoch 13/100\n36/36 - 0s - loss: 1.7737 - accuracy: 0.2815 - val_loss: 1.7513 - val_accuracy: 0.3264 - 353ms/epoch - 10ms/step\nEpoch 14/100\n36/36 - 0s - loss: 1.7734 - accuracy: 0.2815 - val_loss: 1.7509 - val_accuracy: 0.3264 - 369ms/epoch - 10ms/step\nEpoch 15/100\n36/36 - 0s - loss: 1.7735 - accuracy: 0.2815 - val_loss: 1.7517 - val_accuracy: 0.3264 - 366ms/epoch - 10ms/step\nEpoch 16/100\n36/36 - 0s - loss: 1.7738 - accuracy: 0.2815 - val_loss: 1.7516 - val_accuracy: 0.3264 - 355ms/epoch - 10ms/step\nEpoch 17/100\n36/36 - 0s - loss: 1.7735 - accuracy: 0.2815 - val_loss: 1.7502 - val_accuracy: 0.3264 - 344ms/epoch - 10ms/step\nEpoch 18/100\n36/36 - 0s - loss: 1.7732 - accuracy: 0.2815 - val_loss: 1.7516 - val_accuracy: 0.3264 - 420ms/epoch - 12ms/step\nEpoch 19/100\n36/36 - 0s - loss: 1.7731 - accuracy: 0.2815 - val_loss: 1.7510 - val_accuracy: 0.3264 - 349ms/epoch - 10ms/step\nEpoch 20/100\n36/36 - 0s - loss: 1.7735 - accuracy: 0.2815 - val_loss: 1.7498 - val_accuracy: 0.3264 - 341ms/epoch - 9ms/step\nEpoch 21/100\n36/36 - 0s - loss: 1.7729 - accuracy: 0.2815 - val_loss: 1.7502 - val_accuracy: 0.3264 - 341ms/epoch - 9ms/step\nEpoch 22/100\n36/36 - 0s - loss: 1.7731 - accuracy: 0.2815 - val_loss: 1.7520 - val_accuracy: 0.3264 - 384ms/epoch - 11ms/step\nEpoch 23/100\n36/36 - 0s - loss: 1.7729 - accuracy: 0.2815 - val_loss: 1.7503 - val_accuracy: 0.3264 - 372ms/epoch - 10ms/step\nEpoch 24/100\n36/36 - 0s - loss: 1.7733 - accuracy: 0.2815 - val_loss: 1.7497 - val_accuracy: 0.3264 - 359ms/epoch - 10ms/step\nEpoch 25/100\n36/36 - 0s - loss: 1.7734 - accuracy: 0.2815 - val_loss: 1.7491 - val_accuracy: 0.3264 - 398ms/epoch - 11ms/step\nEpoch 26/100\n36/36 - 0s - loss: 1.7728 - accuracy: 0.2815 - val_loss: 1.7508 - val_accuracy: 0.3264 - 362ms/epoch - 10ms/step\nEpoch 27/100\n36/36 - 0s - loss: 1.7734 - accuracy: 0.2815 - val_loss: 1.7498 - val_accuracy: 0.3264 - 357ms/epoch - 10ms/step\nEpoch 28/100\n36/36 - 0s - loss: 1.7731 - accuracy: 0.2815 - val_loss: 1.7502 - val_accuracy: 0.3264 - 464ms/epoch - 13ms/step\nEpoch 29/100\n36/36 - 0s - loss: 1.7728 - accuracy: 0.2815 - val_loss: 1.7507 - val_accuracy: 0.3264 - 395ms/epoch - 11ms/step\nEpoch 30/100\n36/36 - 0s - loss: 1.7730 - accuracy: 0.2815 - val_loss: 1.7501 - val_accuracy: 0.3264 - 385ms/epoch - 11ms/step\nEpoch 31/100\n36/36 - 0s - loss: 1.7731 - accuracy: 0.2815 - val_loss: 1.7489 - val_accuracy: 0.3264 - 347ms/epoch - 10ms/step\nEpoch 32/100\n36/36 - 0s - loss: 1.7727 - accuracy: 0.2815 - val_loss: 1.7506 - val_accuracy: 0.3264 - 379ms/epoch - 11ms/step\nEpoch 33/100\n36/36 - 0s - loss: 1.7729 - accuracy: 0.2815 - val_loss: 1.7506 - val_accuracy: 0.3264 - 351ms/epoch - 10ms/step\nEpoch 34/100\n36/36 - 0s - loss: 1.7730 - accuracy: 0.2815 - val_loss: 1.7493 - val_accuracy: 0.3264 - 422ms/epoch - 12ms/step\nEpoch 35/100\n36/36 - 0s - loss: 1.7731 - accuracy: 0.2815 - val_loss: 1.7504 - val_accuracy: 0.3264 - 354ms/epoch - 10ms/step\nEpoch 36/100\n36/36 - 0s - loss: 1.7730 - accuracy: 0.2815 - val_loss: 1.7505 - val_accuracy: 0.3264 - 341ms/epoch - 9ms/step\nEpoch 37/100\n36/36 - 0s - loss: 1.7728 - accuracy: 0.2815 - val_loss: 1.7500 - val_accuracy: 0.3264 - 347ms/epoch - 10ms/step\nEpoch 38/100\n36/36 - 0s - loss: 1.7728 - accuracy: 0.2815 - val_loss: 1.7500 - val_accuracy: 0.3264 - 346ms/epoch - 10ms/step\nEpoch 39/100\n36/36 - 0s - loss: 1.7726 - accuracy: 0.2815 - val_loss: 1.7510 - val_accuracy: 0.3264 - 339ms/epoch - 9ms/step\nEpoch 40/100\n36/36 - 0s - loss: 1.7730 - accuracy: 0.2815 - val_loss: 1.7507 - val_accuracy: 0.3264 - 338ms/epoch - 9ms/step\nEpoch 41/100\n36/36 - 0s - loss: 1.7727 - accuracy: 0.2815 - val_loss: 1.7495 - val_accuracy: 0.3264 - 346ms/epoch - 10ms/step\nEpoch 42/100\n36/36 - 0s - loss: 1.7728 - accuracy: 0.2815 - val_loss: 1.7510 - val_accuracy: 0.3264 - 356ms/epoch - 10ms/step\nEpoch 43/100\n36/36 - 0s - loss: 1.7732 - accuracy: 0.2815 - val_loss: 1.7501 - val_accuracy: 0.3264 - 416ms/epoch - 12ms/step\nEpoch 44/100\n36/36 - 0s - loss: 1.7730 - accuracy: 0.2815 - val_loss: 1.7501 - val_accuracy: 0.3264 - 356ms/epoch - 10ms/step\nEpoch 45/100\n36/36 - 0s - loss: 1.7735 - accuracy: 0.2815 - val_loss: 1.7496 - val_accuracy: 0.3264 - 338ms/epoch - 9ms/step\nEpoch 46/100\n36/36 - 0s - loss: 1.7732 - accuracy: 0.2815 - val_loss: 1.7510 - val_accuracy: 0.3264 - 354ms/epoch - 10ms/step\nEpoch 47/100\n36/36 - 0s - loss: 1.7730 - accuracy: 0.2815 - val_loss: 1.7504 - val_accuracy: 0.3264 - 341ms/epoch - 9ms/step\nEpoch 48/100\n36/36 - 0s - loss: 1.7733 - accuracy: 0.2815 - val_loss: 1.7503 - val_accuracy: 0.3264 - 412ms/epoch - 11ms/step\nEpoch 49/100\n36/36 - 0s - loss: 1.7734 - accuracy: 0.2815 - val_loss: 1.7510 - val_accuracy: 0.3264 - 415ms/epoch - 12ms/step\nEpoch 50/100\n36/36 - 0s - loss: 1.7728 - accuracy: 0.2815 - val_loss: 1.7502 - val_accuracy: 0.3264 - 342ms/epoch - 9ms/step\nEpoch 51/100\n36/36 - 0s - loss: 1.7732 - accuracy: 0.2815 - val_loss: 1.7500 - val_accuracy: 0.3264 - 444ms/epoch - 12ms/step\nEpoch 52/100\n36/36 - 0s - loss: 1.7730 - accuracy: 0.2815 - val_loss: 1.7500 - val_accuracy: 0.3264 - 333ms/epoch - 9ms/step\nEpoch 53/100\n36/36 - 0s - loss: 1.7737 - accuracy: 0.2815 - val_loss: 1.7501 - val_accuracy: 0.3264 - 335ms/epoch - 9ms/step\nEpoch 54/100\n36/36 - 0s - loss: 1.7731 - accuracy: 0.2815 - val_loss: 1.7523 - val_accuracy: 0.3264 - 373ms/epoch - 10ms/step\nEpoch 55/100\n36/36 - 0s - loss: 1.7732 - accuracy: 0.2815 - val_loss: 1.7498 - val_accuracy: 0.3264 - 424ms/epoch - 12ms/step\nEpoch 56/100\n36/36 - 0s - loss: 1.7728 - accuracy: 0.2815 - val_loss: 1.7511 - val_accuracy: 0.3264 - 357ms/epoch - 10ms/step\nEpoch 57/100\n36/36 - 0s - loss: 1.7734 - accuracy: 0.2815 - val_loss: 1.7507 - val_accuracy: 0.3264 - 453ms/epoch - 13ms/step\nEpoch 58/100\n36/36 - 0s - loss: 1.7733 - accuracy: 0.2815 - val_loss: 1.7508 - val_accuracy: 0.3264 - 407ms/epoch - 11ms/step\nEpoch 59/100\n36/36 - 0s - loss: 1.7731 - accuracy: 0.2815 - val_loss: 1.7503 - val_accuracy: 0.3264 - 357ms/epoch - 10ms/step\nEpoch 60/100\n36/36 - 0s - loss: 1.7730 - accuracy: 0.2815 - val_loss: 1.7499 - val_accuracy: 0.3264 - 349ms/epoch - 10ms/step\nEpoch 61/100\n36/36 - 0s - loss: 1.7734 - accuracy: 0.2815 - val_loss: 1.7492 - val_accuracy: 0.3264 - 366ms/epoch - 10ms/step\nEpoch 62/100\n36/36 - 0s - loss: 1.7731 - accuracy: 0.2815 - val_loss: 1.7496 - val_accuracy: 0.3264 - 402ms/epoch - 11ms/step\nEpoch 63/100\n36/36 - 0s - loss: 1.7729 - accuracy: 0.2815 - val_loss: 1.7524 - val_accuracy: 0.3264 - 352ms/epoch - 10ms/step\nEpoch 64/100\n36/36 - 0s - loss: 1.7726 - accuracy: 0.2815 - val_loss: 1.7496 - val_accuracy: 0.3264 - 375ms/epoch - 10ms/step\nEpoch 65/100\n36/36 - 0s - loss: 1.7728 - accuracy: 0.2815 - val_loss: 1.7499 - val_accuracy: 0.3264 - 336ms/epoch - 9ms/step\nEpoch 66/100\n36/36 - 0s - loss: 1.7730 - accuracy: 0.2815 - val_loss: 1.7501 - val_accuracy: 0.3264 - 342ms/epoch - 10ms/step\nEpoch 67/100\n36/36 - 0s - loss: 1.7735 - accuracy: 0.2815 - val_loss: 1.7497 - val_accuracy: 0.3264 - 364ms/epoch - 10ms/step\nEpoch 68/100\n36/36 - 0s - loss: 1.7730 - accuracy: 0.2815 - val_loss: 1.7506 - val_accuracy: 0.3264 - 335ms/epoch - 9ms/step\nEpoch 69/100\n36/36 - 0s - loss: 1.7735 - accuracy: 0.2815 - val_loss: 1.7509 - val_accuracy: 0.3264 - 427ms/epoch - 12ms/step\nEpoch 70/100\n36/36 - 0s - loss: 1.7723 - accuracy: 0.2815 - val_loss: 1.7488 - val_accuracy: 0.3264 - 414ms/epoch - 12ms/step\nEpoch 71/100\n36/36 - 1s - loss: 1.7733 - accuracy: 0.2815 - val_loss: 1.7496 - val_accuracy: 0.3264 - 512ms/epoch - 14ms/step\nEpoch 72/100\n36/36 - 0s - loss: 1.7729 - accuracy: 0.2815 - val_loss: 1.7503 - val_accuracy: 0.3264 - 425ms/epoch - 12ms/step\nEpoch 73/100\n36/36 - 0s - loss: 1.7728 - accuracy: 0.2815 - val_loss: 1.7495 - val_accuracy: 0.3264 - 359ms/epoch - 10ms/step\nEpoch 74/100\n36/36 - 0s - loss: 1.7733 - accuracy: 0.2815 - val_loss: 1.7503 - val_accuracy: 0.3264 - 348ms/epoch - 10ms/step\nEpoch 75/100\n36/36 - 0s - loss: 1.7731 - accuracy: 0.2815 - val_loss: 1.7518 - val_accuracy: 0.3264 - 359ms/epoch - 10ms/step\nEpoch 76/100\n36/36 - 0s - loss: 1.7730 - accuracy: 0.2815 - val_loss: 1.7499 - val_accuracy: 0.3264 - 322ms/epoch - 9ms/step\nEpoch 77/100\n36/36 - 0s - loss: 1.7739 - accuracy: 0.2815 - val_loss: 1.7507 - val_accuracy: 0.3264 - 417ms/epoch - 12ms/step\nEpoch 78/100\n36/36 - 0s - loss: 1.7733 - accuracy: 0.2815 - val_loss: 1.7504 - val_accuracy: 0.3264 - 352ms/epoch - 10ms/step\nEpoch 79/100\n36/36 - 0s - loss: 1.7740 - accuracy: 0.2815 - val_loss: 1.7488 - val_accuracy: 0.3264 - 343ms/epoch - 10ms/step\nEpoch 80/100\n36/36 - 0s - loss: 1.7730 - accuracy: 0.2815 - val_loss: 1.7493 - val_accuracy: 0.3264 - 460ms/epoch - 13ms/step\nEpoch 81/100\n36/36 - 0s - loss: 1.7729 - accuracy: 0.2815 - val_loss: 1.7496 - val_accuracy: 0.3264 - 341ms/epoch - 9ms/step\nEpoch 82/100\n36/36 - 0s - loss: 1.7736 - accuracy: 0.2815 - val_loss: 1.7513 - val_accuracy: 0.3264 - 348ms/epoch - 10ms/step\nEpoch 83/100\n36/36 - 0s - loss: 1.7734 - accuracy: 0.2815 - val_loss: 1.7487 - val_accuracy: 0.3264 - 375ms/epoch - 10ms/step\nEpoch 84/100\n36/36 - 0s - loss: 1.7735 - accuracy: 0.2815 - val_loss: 1.7511 - val_accuracy: 0.3264 - 419ms/epoch - 12ms/step\nEpoch 85/100\n36/36 - 0s - loss: 1.7729 - accuracy: 0.2815 - val_loss: 1.7507 - val_accuracy: 0.3264 - 342ms/epoch - 9ms/step\nEpoch 86/100\n36/36 - 0s - loss: 1.7734 - accuracy: 0.2815 - val_loss: 1.7508 - val_accuracy: 0.3264 - 434ms/epoch - 12ms/step\nEpoch 87/100\n36/36 - 0s - loss: 1.7734 - accuracy: 0.2815 - val_loss: 1.7512 - val_accuracy: 0.3264 - 403ms/epoch - 11ms/step\nEpoch 88/100\n36/36 - 0s - loss: 1.7737 - accuracy: 0.2815 - val_loss: 1.7506 - val_accuracy: 0.3264 - 428ms/epoch - 12ms/step\nEpoch 89/100\n36/36 - 0s - loss: 1.7728 - accuracy: 0.2815 - val_loss: 1.7511 - val_accuracy: 0.3264 - 348ms/epoch - 10ms/step\nEpoch 90/100\n36/36 - 0s - loss: 1.7732 - accuracy: 0.2815 - val_loss: 1.7499 - val_accuracy: 0.3264 - 352ms/epoch - 10ms/step\nEpoch 91/100\n36/36 - 0s - loss: 1.7728 - accuracy: 0.2815 - val_loss: 1.7502 - val_accuracy: 0.3264 - 352ms/epoch - 10ms/step\nEpoch 92/100\n36/36 - 0s - loss: 1.7728 - accuracy: 0.2815 - val_loss: 1.7507 - val_accuracy: 0.3264 - 344ms/epoch - 10ms/step\nEpoch 93/100\n36/36 - 0s - loss: 1.7733 - accuracy: 0.2815 - val_loss: 1.7492 - val_accuracy: 0.3264 - 343ms/epoch - 10ms/step\nEpoch 94/100\n36/36 - 0s - loss: 1.7729 - accuracy: 0.2815 - val_loss: 1.7504 - val_accuracy: 0.3264 - 325ms/epoch - 9ms/step\nEpoch 95/100\n36/36 - 0s - loss: 1.7729 - accuracy: 0.2815 - val_loss: 1.7502 - val_accuracy: 0.3264 - 330ms/epoch - 9ms/step\nEpoch 96/100\n36/36 - 0s - loss: 1.7728 - accuracy: 0.2815 - val_loss: 1.7510 - val_accuracy: 0.3264 - 418ms/epoch - 12ms/step\nEpoch 97/100\n36/36 - 0s - loss: 1.7730 - accuracy: 0.2815 - val_loss: 1.7496 - val_accuracy: 0.3264 - 339ms/epoch - 9ms/step\nEpoch 98/100\n36/36 - 0s - loss: 1.7727 - accuracy: 0.2815 - val_loss: 1.7503 - val_accuracy: 0.3264 - 335ms/epoch - 9ms/step\nEpoch 99/100\n36/36 - 0s - loss: 1.7731 - accuracy: 0.2815 - val_loss: 1.7498 - val_accuracy: 0.3264 - 416ms/epoch - 12ms/step\nEpoch 100/100\n36/36 - 0s - loss: 1.7733 - accuracy: 0.2815 - val_loss: 1.7506 - val_accuracy: 0.3264 - 416ms/epoch - 12ms/step\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7fecdc1d87d0>"},"metadata":{}}]}]}